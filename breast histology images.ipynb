{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras.preprocessing.image\nimport sklearn.preprocessing\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.linear_model\nimport sklearn.naive_bayes\nimport sklearn.tree\nimport sklearn.ensemble\nimport os;\nimport datetime  \nimport cv2 \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm  \n%matplotlib inline\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=np.load('../input/X.npy')\ny=np.load('../input/Y.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#l=np.arange(len(y))\nstate=np.random.get_state()\nnp.random.shuffle(x)\n#x=x[l]\n#y=y[l]\nnp.random.set_state(state)\nnp.random.shuffle(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x.dtype)   #uint8\nprint(y.dtype)   #int64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#0-255 --> 0-1\nx_train_valid=x.astype('float32')/255.0\n#print(x_train_valid[0,:,:,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_dense(onehot):\n    return np.argmax(onehot,1)\n\ndef dense_onehot(dense,num_class):\n    return np.eye(num_class)[dense]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_valid=dense_onehot(y,2).astype('uint8')  #从int64-->uint8，节省存储空间\n#print(y_train_valid[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class nn():\n    def __init__(self,nn_name='tmp',log_step=0.1,keep_prob=0.33,bs=50,w=50,h=50,n_channel=3,n_output=2):\n        # tunable hyperparameters for nn architecture\n        self.s_f_conv1 = 3 # size of filter of first convolution layer (default = 3)   过滤器边长\n        self.n_f_conv1 = 36 # number of features of first convolution layer (default = 36)\n        self.s_f_conv2 = 3 # filter size of second convolution layer (default = 3)\n        self.n_f_conv2 = 36 # number of features of second convolution layer (default = 36)\n        self.s_f_conv3 = 3 # filter size of third convolution layer (default = 3)\n        self.n_f_conv3 = 36 # number of features of third convolution layer (default = 36)\n        self.n_n_fc1 = 576 # number of neurons of first fully connected layer (default = 576)\n        self.n_channel = n_channel\n        self.width = w\n        self.height = h\n        self.n_output = n_output\n        \n        # tunable hyperparameters for training\n        self.mb_size = bs # mini batch size\n        self.keep_prob = keep_prob # keeping probability with dropout regularization \n        self.learn_rate_array = [10*1e-4, 7.5*1e-4, 5*1e-4, 2.5*1e-4, 1*1e-4, 1*1e-4,\n                                 1*1e-4, 0.75*1e-4, 0.5*1e-4, 0.25*1e-4, 0.1*1e-4, \n                                 0.1*1e-4, 0.075*1e-4,0.050*1e-4, 0.025*1e-4, 0.01*1e-4, \n                                 0.0075*1e-4, 0.0050*1e-4,0.0025*1e-4,0.001*1e-4]\n        self.learn_rate_step_size = 3 # in terms of epochs              在矩阵中每次位移\n        \n        # helper variables\n        self.learn_rate = self.learn_rate_array[0]\n        self.learn_rate_pos = 0 # current position pointing to current learning rate  \n        #self.learn_rate_pos = int(self.current_epoch // self.learn_rate_step_size)\n        self.index_in_epoch = 0  #当前epoch中已经执行了多少个x[,:]                                                \n        self.current_epoch = 0   #执行到第几个epoch的百分之几了 \n        self.log_step = log_step # log results in terms of epochs    绘图用，不了解                 ??????\n        self.n_log_step = 0 # counting current number of mini batches trained on  第几次写入tensorboard\n        self.use_tb_summary = False # True = use tensorboard visualization 写入ts权限\n        self.use_tf_saver = False # True = use saver to save the model     存储权限\n        self.nn_name = nn_name # name of the neural network\n        self.perm_array = np.array([]) # permutation array  打乱生成batch顺序              \n        \n    \"\"\"\n    # weight initialization\n    def weight_variable(self, shape, name = None):\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name = name)\n        #tf.Variable(tf.truncated_normal(shape, stddev=0.1), name = name)\n    \n    # bias initialization\n    def bias_variable(self, shape, name = None):\n        initial = tf.constant(0.1, shape=shape) #  positive bias\n        return tf.Variable(initial, name = name)\n        #tf.Variable(tf.constant(0.1, shape), name = name)\n    \"\"\"\n    # 2D convolution\n    def conv2d(self, x, W, name = None):\n        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name = name)\n\n    # max pooling\n    def max_pool_2x2(self, x, name = None):\n        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding='SAME', name = name)\n            \n    # function to create the graph\n    def create_graph(self):\n\n        # reset default graph\n        tf.reset_default_graph()\n\n        # variables for input and output \n        self.x_data_tf = tf.placeholder(dtype=tf.float32, shape=[None,self.height,\n                                        self.width,self.n_channel], name='x_data_tf')\n        self.y_data_tf = tf.placeholder(dtype=tf.float32, shape=[None, self.n_output],\n                                        name='y_data_tf')\n\n        # 1.layer: convolution + max pooling\n        self.W_conv1_tf =tf.Variable(tf.truncated_normal([self.s_f_conv1, self.s_f_conv1, self.n_channel,\n                                                self.n_f_conv1], stddev=0.1), name ='W_conv1_tf')# (3,3,3,36)\n        self.b_conv1_tf =tf.Variable(tf.constant(0.1, shape=[self.n_f_conv1]), name = 'b_conv1_tf')# (36)\n        self.h_conv1_tf = tf.nn.relu(self.conv2d(self.x_data_tf, self.W_conv1_tf) \n                                     + self.b_conv1_tf, name = 'h_conv1_tf') # (.,50,50,36)\n        self.h_pool1_tf = self.max_pool_2x2(self.h_conv1_tf, \n                                            name = 'h_pool1_tf') # (.,25,25,36)\n        \n        # 2.layer: convolution + max pooling\n        self.W_conv2_tf =tf.Variable(tf.truncated_normal([self.s_f_conv2, self.s_f_conv2, self.n_f_conv1,\n                                                self.n_f_conv2], stddev=0.1), name ='W_conv2_tf')\n        self.b_conv2_tf =tf.Variable(tf.constant(0.1, shape=[self.n_f_conv2]), name = 'b_conv2_tf')\n        self.h_conv2_tf = tf.nn.relu(self.conv2d(self.h_pool1_tf, \n                        self.W_conv2_tf) + self.b_conv2_tf, name ='h_conv2_tf') #(.,25,25,36)\n        self.h_pool2_tf = self.max_pool_2x2(self.h_conv2_tf, name = 'h_pool2_tf') #(.,13,13,36)\n\n        # 3.layer: convolution + max pooling\n        self.W_conv3_tf =tf.Variable(tf.truncated_normal([self.s_f_conv3, self.s_f_conv3, self.n_f_conv2,\n                                                self.n_f_conv3], stddev=0.1), name ='W_conv3_tf')\n        self.b_conv3_tf =tf.Variable(tf.constant(0.1, shape=[self.n_f_conv3]), name = 'b_conv3_tf')\n        self.h_conv3_tf = tf.nn.relu(self.conv2d(self.h_pool2_tf, self.W_conv3_tf) + \n                                     self.b_conv3_tf, name ='h_conv3_tf') #(.,13,13,36)\n        self.h_pool3_tf = self.max_pool_2x2(self.h_conv3_tf, name='h_pool3_tf') # (.,7,7,36)\n        \n        # 4.layer: fully connected  写的不伦不类，7应该由shape读取\n        self.W_fc1_tf =tf.Variable(tf.truncated_normal([7*7*self.n_f_conv3, self.n_n_fc1], stddev=0.1), name ='W_fc1_tf') # (7*7*36, 1024)\n        self.b_fc1_tf =tf.Variable(tf.constant(0.1, shape=[self.n_n_fc1]), name = 'b_fc1_tf')# (1024)\n        self.h_pool3_flat_tf = tf.reshape(self.h_pool3_tf, [-1,7*7*self.n_f_conv3], \n                                          name = 'h_pool3_flat_tf') # (.,1024)\n        self.h_fc1_tf = tf.nn.relu(tf.matmul(self.h_pool3_flat_tf, \n                           self.W_fc1_tf) + self.b_fc1_tf, name = 'h_fc1_tf') # (.,1024)\n      \n        # add dropout\n        self.keep_prob_tf = tf.placeholder(dtype=tf.float32, name = 'keep_prob_tf')\n        self.h_fc1_drop_tf = tf.nn.dropout(self.h_fc1_tf, self.keep_prob_tf, name = 'h_fc1_drop_tf')\n\n        # 5.layer: fully connected\n        self.W_fc2_tf =tf.Variable(tf.truncated_normal([self.n_n_fc1, self.n_output], stddev=0.1), name ='W_fc2_tf')# (1024,1)\n        self.b_fc2_tf =tf.Variable(tf.constant(0.1, shape=[self.n_output]), name = 'b_fc2_tf')# (1024)\n        self.z_pred_tf = tf.add(tf.matmul(self.h_fc1_drop_tf, self.W_fc2_tf), \n                                self.b_fc2_tf, name = 'z_pred_tf')# => (.,1)\n\n        # cost function loss函数\n        self.cross_entropy_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n            labels=self.y_data_tf, logits=self.z_pred_tf), name = 'cross_entropy_tf')\n     \n        # optimisation function 训练，学习率从矩阵中选择\n        self.learn_rate_tf = tf.placeholder(dtype=tf.float32, name=\"learn_rate_tf\")\n        self.train_step_tf = tf.train.AdamOptimizer(self.learn_rate_tf).minimize(\n            self.cross_entropy_tf, name = 'train_step_tf')\n\n        # predicted probabilities in one-hot encoding\n        self.y_pred_proba_tf = tf.nn.softmax(self.z_pred_tf, name='y_pred_proba_tf') \n        \n        # tensor of correct predictions \n        self.y_pred_correct_tf = tf.equal(tf.argmax(self.y_pred_proba_tf, 1),\n                                          tf.argmax(self.y_data_tf, 1),\n                                          name = 'y_pred_correct_tf')  \n        \n        # accuracy  准确率\n        self.accuracy_tf = tf.reduce_mean(tf.cast(self.y_pred_correct_tf, dtype=tf.float32),\n                                         name = 'accuracy_tf')\n\n        # tensors to save intermediate accuracies and losses during training 存储loss，注意shape可变\n        self.train_loss_tf = tf.Variable(np.array([]), dtype=tf.float32, \n                                         name='train_loss_tf', validate_shape = False)\n        self.valid_loss_tf = tf.Variable(np.array([]), dtype=tf.float32, \n                                         name='valid_loss_tf', validate_shape = False)\n        self.train_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n                                        name='train_acc_tf', validate_shape = False)\n        self.valid_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n                                        name='valid_acc_tf', validate_shape = False)\n     \n        # number of weights and biases 参数数量，不知道有什么用\n        num_weights = (self.s_f_conv1**2*self.n_f_conv1*self.n_channel\n                       + self.s_f_conv2**2*self.n_f_conv1*self.n_f_conv2 \n                       + self.s_f_conv3**2*self.n_f_conv2*self.n_f_conv3 \n                       + 4*4*self.n_f_conv3*self.n_n_fc1 + self.n_n_fc1*self.n_output)\n        num_biases = self.n_f_conv1 + self.n_f_conv2 + self.n_f_conv3 + self.n_n_fc1\n        print('num_weights =', num_weights)\n        print('num_biases =', num_biases)\n        \n        return None\n    \n    # attach summaries to a tensor for TensorBoard visualization 将均值、标准差、最值、分布写入tensorboard\n    def summary_variable(self, var, var_name):\n        with tf.name_scope(var_name):\n            mean = tf.reduce_mean(var)\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n            tf.summary.scalar('mean', mean)\n            tf.summary.scalar('stddev', stddev)\n            tf.summary.scalar('max', tf.reduce_max(var))\n            tf.summary.scalar('min', tf.reduce_min(var))\n            tf.summary.histogram('histogram', var)\n    \n    def attach_summary(self, sess):        \n        # create summary tensors for tensorboard 开启将权重、loss、acc写入tensorboard的权限\n        self.use_tb_summary = True  #一开始是false\n        self.summary_variable(self.W_conv1_tf, 'W_conv1_tf')\n        self.summary_variable(self.b_conv1_tf, 'b_conv1_tf')\n        self.summary_variable(self.W_conv2_tf, 'W_conv2_tf')\n        self.summary_variable(self.b_conv2_tf, 'b_conv2_tf')\n        self.summary_variable(self.W_conv3_tf, 'W_conv3_tf')\n        self.summary_variable(self.b_conv3_tf, 'b_conv3_tf')\n        self.summary_variable(self.W_fc1_tf, 'W_fc1_tf')\n        self.summary_variable(self.b_fc1_tf, 'b_fc1_tf')\n        self.summary_variable(self.W_fc2_tf, 'W_fc2_tf')\n        self.summary_variable(self.b_fc2_tf, 'b_fc2_tf')\n        tf.summary.scalar('cross_entropy_tf', self.cross_entropy_tf)\n        tf.summary.scalar('accuracy_tf', self.accuracy_tf)\n\n        # merge all summaries for tensorboard  为什么要单独写这个？我注释掉了！\n        #self.merged = tf.summary.merge_all()\n\n        # initialize summary writer \n        timestamp = datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n        filepath = os.path.join(os.getcwd(), 'logs', (self.nn_name+'_'+timestamp))\n        self.train_writer = tf.summary.FileWriter(os.path.join(filepath,'train'), sess.graph)\n        self.valid_writer = tf.summary.FileWriter(os.path.join(filepath,'valid'), sess.graph)\n        \n    # generate new images via rotations, translations, zoom using keras 数据增强\n    def generate_images(self, imgs):  \n        print('generate new set of images')\n        \n        # rotations, translations, zoom\n        image_generator = keras.preprocessing.image.ImageDataGenerator(\n            rotation_range = 10, width_shift_range = 0.1 , height_shift_range = 0.1,\n            zoom_range = 0.1)\n\n        # get transformed images 一次读入所有的imgs，注意.next方法即可生成所需数据\n        images = image_generator.flow(imgs.copy(),batch_size=len(imgs), shuffle = False).next()    \n        # np.zeros(len(imgs)),多余的\n        return images #[0]\n        \n    # function to get the next mini batch\n    def next_mini_batch(self):\n        #start end是从打乱后的数据中抽取的首尾范围\n        start = self.index_in_epoch\n        self.index_in_epoch += self.mb_size\n        self.current_epoch +=self.mb_size/len(self.x_train)#没有问题  \n        \n        # adapt length of permutation array\n        if len(self.perm_array) != len(self.x_train):\n            self.perm_array = np.arange(len(self.x_train))        \n        # shuffle once at the start of epoch\n        if start == 0:\n            np.random.shuffle(self.perm_array)\n\n        # at the end of the epoch  新epoch中第一轮\n        if self.index_in_epoch > self.x_train.shape[0]:\n            np.random.shuffle(self.perm_array) # shuffle data\n            start = 0 # start next epoch\n            self.index_in_epoch = self.mb_size # set index to mini batch size   ???那下一个epoch怎么办，不把iie归零\n            \n            #这里有缩进！ 这一步不是多余的，因为每个epoch都要用新的aug数据   \n            if self.train_on_augmented_data:\n                # use augmented data for the next epoch\n                self.x_train_aug = self.generate_images(self.x_train) #normalize，除以255\n                self.y_train_aug = self.y_train\n               \n        end = self.index_in_epoch      #没有问题，相比start时iie已经改变了\n        \n        if self.train_on_augmented_data:\n            # use augmented data\n            x_tr = self.x_train_aug[self.perm_array[start:end]]\n            y_tr = self.y_train_aug[self.perm_array[start:end]]\n        else:\n        \n            # use original data\n            x_tr = self.x_train[self.perm_array[start:end]]\n            y_tr = self.y_train[self.perm_array[start:end]]\n        \n        return x_tr, y_tr\n    \n    # function to train the graph\n    def train_graph(self, sess, x_train, y_train, x_valid, y_valid, n_epoch = 1, train_on_augmented_data = False):\n        # train on original or augmented data\n        self.train_on_augmented_data = train_on_augmented_data\n        \n        # training and validation data\n        self.x_train = x_train\n        self.y_train = y_train\n        self.x_valid = x_valid\n        self.y_valid = y_valid\n        \n        #下面这段话不是多余的\n        \n        # use augmented data\n        if self.train_on_augmented_data:\n            print('generate new set of images')\n            self.x_train_aug = self.generate_images(self.x_train)\n            self.y_train_aug = self.y_train\n        \n        \n        # parameters\n        mb_per_epoch = self.x_train.shape[0]/self.mb_size\n        train_loss, train_acc, valid_loss, valid_acc = [],[],[],[]\n        #存的应该是写入tensorboard的\n        \n        # start timer\n        start = datetime.datetime.now();\n        print(datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S'),': start training')\n        print('learnrate =',self.learn_rate,', n_epoch =', n_epoch,\n              ', mb_size =', self.mb_size, ', nn_name =', self.nn_name)\n        # looping over mini batches\n        for i in range(int(n_epoch*mb_per_epoch)+1):   #为什么+1\n\n            # adapt learn_rate\n            if self.learn_rate_pos != int(self.current_epoch // self.learn_rate_step_size):\n                self.learn_rate_pos = int(self.current_epoch // self.learn_rate_step_size)\n                self.learn_rate = self.learn_rate_array[self.learn_rate_pos]\n                print(datetime.datetime.now()-start,': set learn rate to %.6f'%self.learn_rate)\n            \n            # get new batch\n            x_batch, y_batch = self.next_mini_batch() \n\n            # run the graph\n            sess.run(self.train_step_tf, feed_dict={self.x_data_tf: x_batch, \n                                                    self.y_data_tf: y_batch, \n                                                    self.keep_prob_tf: self.keep_prob, \n                                                    self.learn_rate_tf: self.learn_rate})\n            # store losses and accuracies  一个是和log相关之倍数，另一个是最后一步\n            if i%int(self.log_step*mb_per_epoch) == 0 or i == int(n_epoch*mb_per_epoch):\n             \n                self.n_log_step += 1 # for logging the results\n                \n                feed_dict_train = {\n                    self.x_data_tf: self.x_train[self.perm_array[:len(self.x_valid)]],   #len有问题？为什么是抽取？ \n                    self.y_data_tf: self.y_train[self.perm_array[:len(self.y_valid)]], \n                    self.keep_prob_tf: 1.0}\n                \n                feed_dict_valid = {self.x_data_tf: self.x_valid, \n                                   self.y_data_tf: self.y_valid, \n                                   self.keep_prob_tf: 1.0}\n                \n                # summary for tensorboard\n                if self.use_tb_summary:\n                    train_summary = sess.run(tf.summary.merge_all(), feed_dict = feed_dict_train)\n                    valid_summary = sess.run(tf.summary.merge_all(), feed_dict = feed_dict_valid)\n                    self.train_writer.add_summary(train_summary, self.n_log_step)\n                    self.valid_writer.add_summary(valid_summary, self.n_log_step)\n                \n                train_loss.append(sess.run(self.cross_entropy_tf,\n                                           feed_dict = feed_dict_train))\n\n                train_acc.append(self.accuracy_tf.eval(session = sess, \n                                                       feed_dict = feed_dict_train))\n                \n                valid_loss.append(sess.run(self.cross_entropy_tf,\n                                           feed_dict = feed_dict_valid))\n\n                valid_acc.append(self.accuracy_tf.eval(session = sess, \n                                                       feed_dict = feed_dict_valid))\n\n                print('%.2f epoch: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(\n                    self.current_epoch, train_loss[-1], valid_loss[-1],\n                    train_acc[-1], valid_acc[-1]))\n     \n        # concatenate losses and accuracies and assign to tensor variables\n        #没搞懂为什么这么做，train-loss是空的怎么办???????????\n        \n        tl_c = np.concatenate([self.train_loss_tf.eval(session=sess), train_loss], axis = 0)\n        vl_c = np.concatenate([self.valid_loss_tf.eval(session=sess), valid_loss], axis = 0)\n        ta_c = np.concatenate([self.train_acc_tf.eval(session=sess), train_acc], axis = 0)\n        va_c = np.concatenate([self.valid_acc_tf.eval(session=sess), valid_acc], axis = 0)\n        sess.run(tf.assign(self.train_loss_tf,tl_c, validate_shape = False))\n        sess.run(tf.assign(self.valid_loss_tf, vl_c , validate_shape = False))\n        sess.run(tf.assign(self.train_acc_tf, ta_c , validate_shape = False))\n        sess.run(tf.assign(self.valid_acc_tf,  va_c , validate_shape = False))\n        \"\"\"#只用这四行好像也可以\n        sess.run(tf.assign(self.train_loss_tf,train_loss, validate_shape = False))\n        sess.run(tf.assign(self.valid_loss_tf, valid_loss , validate_shape = False))\n        sess.run(tf.assign(self.train_acc_tf, train_acc , validate_shape = False))\n        sess.run(tf.assign(self.valid_acc_tf,  valid_acc , validate_shape = False))\n        \"\"\"\n        print('running time for training: ', datetime.datetime.now() - start)\n        return None\n    \n    def attach_saver(self):\n        # initialize tensorflow saver\n        self.use_tf_saver = True     #self.saver_tf = tf.train.Saver()  #冗余，注释掉了！\n    \n    # save tensors/summaries\n    def save_model(self, sess):\n        \n        # tf saver\n        if self.use_tf_saver:\n            #filepath = os.path.join(os.getcwd(), 'logs' , self.nn_name)\n            filepath = os.path.join(os.getcwd(), self.nn_name)\n            tf.train.Saver().save(sess, filepath)\n        \n        # tb summary\n        if self.use_tb_summary:\n            self.train_writer.close()\n            self.valid_writer.close()\n        \n        return None\n    \n     # forward prediction of current graph\n     #冗余的一个函数！\n    def forward(self, sess, x_data):\n        y_pred_proba = self.y_pred_proba_tf.eval(session = sess, \n                                                 feed_dict = {self.x_data_tf: x_data,\n                                                              self.keep_prob_tf: 1.0})\n        return y_pred_proba\n    \n    # function to load tensors from a saved graph 就是要get_tensor_by_name\n    def load_tensors(self, graph):        \n        # input tensors\n        self.x_data_tf = graph.get_tensor_by_name(\"x_data_tf:0\")\n        self.y_data_tf = graph.get_tensor_by_name(\"y_data_tf:0\")\n        \n        # weights and bias tensors\n        self.W_conv1_tf = graph.get_tensor_by_name(\"W_conv1_tf:0\")\n        self.W_conv2_tf = graph.get_tensor_by_name(\"W_conv2_tf:0\")\n        self.W_conv3_tf = graph.get_tensor_by_name(\"W_conv3_tf:0\")\n        self.W_fc1_tf = graph.get_tensor_by_name(\"W_fc1_tf:0\")\n        self.W_fc2_tf = graph.get_tensor_by_name(\"W_fc2_tf:0\")\n        self.b_conv1_tf = graph.get_tensor_by_name(\"b_conv1_tf:0\")\n        self.b_conv2_tf = graph.get_tensor_by_name(\"b_conv2_tf:0\")\n        self.b_conv3_tf = graph.get_tensor_by_name(\"b_conv3_tf:0\")\n        self.b_fc1_tf = graph.get_tensor_by_name(\"b_fc1_tf:0\")\n        self.b_fc2_tf = graph.get_tensor_by_name(\"b_fc2_tf:0\")\n        \n        # activation tensors\n        self.h_conv1_tf = graph.get_tensor_by_name('h_conv1_tf:0')  \n        self.h_pool1_tf = graph.get_tensor_by_name('h_pool1_tf:0')\n        self.h_conv2_tf = graph.get_tensor_by_name('h_conv2_tf:0')\n        self.h_pool2_tf = graph.get_tensor_by_name('h_pool2_tf:0')\n        self.h_conv3_tf = graph.get_tensor_by_name('h_conv3_tf:0')\n        self.h_pool3_tf = graph.get_tensor_by_name('h_pool3_tf:0')\n        self.h_fc1_tf = graph.get_tensor_by_name('h_fc1_tf:0')\n        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n        \n        # training and prediction tensors\n        self.learn_rate_tf = graph.get_tensor_by_name(\"learn_rate_tf:0\")\n        self.keep_prob_tf = graph.get_tensor_by_name(\"keep_prob_tf:0\")\n        self.cross_entropy_tf = graph.get_tensor_by_name('cross_entropy_tf:0')\n        self.train_step_tf = graph.get_operation_by_name('train_step_tf')\n        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n        self.y_pred_proba_tf = graph.get_tensor_by_name(\"y_pred_proba_tf:0\")\n        self.y_pred_correct_tf = graph.get_tensor_by_name('y_pred_correct_tf:0')\n        self.accuracy_tf = graph.get_tensor_by_name('accuracy_tf:0')\n        \n        # tensor of stored losses and accuricies during training\n        self.train_loss_tf = graph.get_tensor_by_name(\"train_loss_tf:0\")\n        self.train_acc_tf = graph.get_tensor_by_name(\"train_acc_tf:0\")\n        self.valid_loss_tf = graph.get_tensor_by_name(\"valid_loss_tf:0\")\n        self.valid_acc_tf = graph.get_tensor_by_name(\"valid_acc_tf:0\")\n  \n        return None\n    \n    # load session from file, restore graph, and load tensors\n    def load_session_from_file(self, filename):\n        tf.reset_default_graph()\n        filepath = os.path.join(os.getcwd(), filename + '.meta')\n        #filepath = os.path.join(os.getcwd(),'logs', filename + '.meta')\n        saver = tf.train.import_meta_graph(filepath)\n        print(filepath)\n        sess = tf.Session()\n        saver.restore(sess, mn)\n        graph = tf.get_default_graph()\n        self.load_tensors(graph)\n        return sess\n    \n    # get losses of training and validation sets\n    #感觉也很冗余的一个函数！\n    def get_loss(self, sess):\n        train_loss = self.train_loss_tf.eval(session = sess)\n        valid_loss = self.valid_loss_tf.eval(session = sess)\n        return train_loss, valid_loss \n        \n    # get accuracies of training and validation sets\n    #感觉也很冗余的一个函数！\n    def get_accuracy(self, sess):\n        train_acc = self.train_acc_tf.eval(session = sess)\n        valid_acc = self.valid_acc_tf.eval(session = sess)\n        return train_acc, valid_acc \n    \n    # get weights\n    #感觉也很冗余的一个函数！\n    def get_weights(self, sess):\n        W_conv1 = self.W_conv1_tf.eval(session = sess)\n        W_conv2 = self.W_conv2_tf.eval(session = sess)\n        W_conv3 = self.W_conv3_tf.eval(session = sess)\n        W_fc1_tf = self.W_fc1_tf.eval(session = sess)\n        W_fc2_tf = self.W_fc2_tf.eval(session = sess)\n        return W_conv1, W_conv2, W_conv3, W_fc1_tf, W_fc2_tf\n    \n    # get biases\n    #感觉也很冗余的一个函数！\n    def get_biases(self, sess):\n        b_conv1 = self.b_conv1_tf.eval(session = sess)\n        b_conv2 = self.b_conv2_tf.eval(session = sess)\n        b_conv3 = self.b_conv3_tf.eval(session = sess)\n        b_fc1_tf = self.b_fc1_tf.eval(session = sess)\n        b_fc2_tf = self.b_fc2_tf.eval(session = sess)\n        return b_conv1, b_conv2, b_conv3, b_fc1_tf, b_fc2_tf\n    \n    \n    # receive activations given the input\n    #感觉也很冗余的一个函数！\n    def get_activations(self, sess, x_data):\n        feed_dict = {self.x_data_tf: x_data, self.keep_prob_tf: 1.0}\n        h_conv1 = self.h_conv1_tf.eval(session = sess, feed_dict = feed_dict)\n        h_pool1 = self.h_pool1_tf.eval(session = sess, feed_dict = feed_dict)\n        h_conv2 = self.h_conv2_tf.eval(session = sess, feed_dict = feed_dict)\n        h_pool2 = self.h_pool2_tf.eval(session = sess, feed_dict = feed_dict)\n        h_conv3 = self.h_conv3_tf.eval(session = sess, feed_dict = feed_dict)\n        h_pool3 = self.h_pool3_tf.eval(session = sess, feed_dict = feed_dict)\n        h_fc1 = self.h_fc1_tf.eval(session = sess, feed_dict = feed_dict)\n        h_fc2 = self.z_pred_tf.eval(session = sess, feed_dict = feed_dict)\n        return h_conv1,h_pool1,h_conv2,h_pool2,h_conv3,h_pool3,h_fc1,h_fc2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## train the neural network graph\n\nnn_name = ['nn0','nn1','nn2','nn3','nn4','nn5','nn6','nn7','nn8','nn9']\n\n# cross validations\ncv_num = 10 # cross validations default = 20 => 5% validation set\nkfold = sklearn.model_selection.KFold(cv_num, shuffle=True, random_state=123)\n\nfor i,(train_index, valid_index) in enumerate(kfold.split(x_train_valid)):\n    \n    # start timer\n    start = datetime.datetime.now();\n    \n    # train and validation data of original images\n    x_train = x_train_valid[train_index]\n    y_train = y_train_valid[train_index]\n    x_valid = x_train_valid[valid_index]\n    y_valid = y_train_valid[valid_index]\n    \n    # create neural network graph\n    nn_graph = nn(nn_name = nn_name[i]) # instance of nn_class 初始化参数\n    nn_graph.create_graph() # create graph 搭网络\n    nn_graph.attach_saver() # attach saver tensors  允许存储\n    \n    # start tensorflow session\n    with tf.Session() as sess:\n        \n        # attach summaries\n        nn_graph.attach_summary(sess)  #哪些写入ts\n        \n        # variable initialization of the default graph\n        sess.run(tf.global_variables_initializer()) \n    \n        # training on original data\n        nn_graph.train_graph(sess, x_train, y_train, x_valid, y_valid, n_epoch = 1.)  #训练\n        \n        # training on augmented data\n        nn_graph.train_graph(sess, x_train, y_train, x_valid, y_valid, n_epoch = 29.,\n                             train_on_augmented_data = True)  #训练\n\n        # save tensors and summaries of model\n        nn_graph.save_model(sess)  #保存\n    \n    break\n    \nprint('total running time for training: ', datetime.datetime.now() - start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## visualization with tensorboard\nif False: #?\n    !tensorboard --logdir=./logs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_valid_pred = {}\ny_train_pred = {}\ny_test_pred = {}\ntrain_loss, valid_loss = {}, {}\ntrain_acc, valid_acc = {}, {}\ncnf_valid_matrix = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## loss and accuracy curves\n\n# choose neural network\nmn = nn_name[0]\nnn_graph = nn()\nsess = nn_graph.load_session_from_file(mn)\ntrain_loss[mn], valid_loss[mn] = nn_graph.get_loss(sess)\ntrain_acc[mn], valid_acc[mn] = nn_graph.get_accuracy(sess)\nsess.close()\n\nprint('final train/valid loss = %.4f/%.4f, train/valid accuracy = %.4f/%.4f'%(\n    train_loss[mn][-1], valid_loss[mn][-1], train_acc[mn][-1], valid_acc[mn][-1]))\n\nplt.figure(figsize=(10, 5));\nplt.subplot(1,2,1);\nplt.plot(np.arange(0,len(train_acc[mn])), train_acc[mn],'-b', label='Training')\nplt.plot(np.arange(0,len(valid_acc[mn])), valid_acc[mn],'-g', label='Validation')\nplt.legend(loc='lower right', frameon=False)\nplt.ylim(ymax = 1.1, ymin = 0.0)\nplt.ylabel('accuracy')\nplt.xlabel('log steps');\n\nplt.subplot(1,2,2)\nplt.plot(np.arange(0,len(train_loss[mn])), train_loss[mn],'-b', label='Training')\nplt.plot(np.arange(0,len(valid_loss[mn])), valid_loss[mn],'-g', label='Validation')\nplt.legend(loc='lower right', frameon=False)\nplt.ylim(ymax = 3.0, ymin = 0.0)\nplt.ylabel('loss')\nplt.xlabel('log steps');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## show confusion matrix\n\n# choose neural network\nmn = nn_name[0]\nnn_graph = nn()\nsess = nn_graph.load_session_from_file(mn)\ny_valid_pred[mn] = nn_graph.forward(sess, x_valid)\nsess.close()\n\n# confusion matrix\ncnf_valid_matrix['neural_network'] = sklearn.metrics.confusion_matrix(\n    y_pred = onehot_dense(y_valid_pred[mn]), \n    y_true = onehot_dense(y_valid)).astype(np.float32)\n\n# normalize\ncnf_valid_matrix['neural_network'][0,:] = cnf_valid_matrix['neural_network'][0,:]/cnf_valid_matrix['neural_network'][0,:].sum()  \ncnf_valid_matrix['neural_network'][1,:] = cnf_valid_matrix['neural_network'][1,:]/cnf_valid_matrix['neural_network'][1,:].sum()  \n\n# plot\nlabels_array = ['IDC = 0', 'IDC = 1']\nfig, ax = plt.subplots(1,figsize=(5,5))\nax = sns.heatmap(cnf_valid_matrix['neural_network'], ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(labels_array)\nax.set_yticklabels(labels_array)\nplt.title('Confusion matrix of validation set')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show();","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}